(window.webpackJsonp=window.webpackJsonp||[]).push([[8],{214:function(e,r,t){"use strict";t.r(r);var n=t(0),a=Object(n.a)({},(function(){var e=this,r=e.$createElement,t=e._self._c||r;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("ProfileSection",{attrs:{frontmatter:e.$page.frontmatter}}),e._v(" "),t("h2",{attrs:{id:"about-me"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#about-me"}},[e._v("#")]),e._v(" About Me")]),e._v(" "),t("p",[e._v("Hi there! I am a joint Ph.D. student of "),t("a",{attrs:{href:"http://en.hit.edu.cn/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Harbin Institute of Technology"),t("OutboundLink")],1),e._v("  and "),t("a",{attrs:{href:"https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Microsoft Research Asia"),t("OutboundLink")],1),e._v(", supervised by Prof. Xiangzhan Yu and Dr. Ming Zhou.\nI obtained my B.S. degree at Harbin Institute of Technology, supervised by "),t("a",{attrs:{href:"http://ir.hit.edu.cn/~car/zh/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Prof. Wanxiang Che"),t("OutboundLink")],1),e._v(".\nCurrently, I am working as a research intern at Natural Language Computing Group in MSRA, mentored by "),t("a",{attrs:{href:"https://www.microsoft.com/en-us/research/people/yuwu1/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Dr. Yu Wu"),t("OutboundLink")],1),e._v(" and "),t("a",{attrs:{href:"https://www.microsoft.com/en-us/research/people/shujliu/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Dr. Shujie Liu"),t("OutboundLink")],1),e._v(".")]),e._v(" "),t("p",[e._v("My current research focuses on "),t("strong",[e._v("language model pre-training for speech and audio processing")]),e._v(",\nand I have developed several pre-trained models, including "),t("a",{attrs:{href:"https://arxiv.org/abs/2301.02111",target:"_blank",rel:"noopener noreferrer"}},[e._v("VALL-E"),t("OutboundLink")],1),e._v(", "),t("a",{attrs:{href:"https://ieeexplore.ieee.org/document/9814838",target:"_blank",rel:"noopener noreferrer"}},[e._v("WavLM"),t("OutboundLink")],1),e._v(" and "),t("a",{attrs:{href:"https://arxiv.org/abs/2212.09058",target:"_blank",rel:"noopener noreferrer"}},[e._v("BEATs"),t("OutboundLink")],1),e._v(",\nthat have advanced the state of the art on various public benchmarks and contributed to the research field.")]),e._v(" "),t("p",[e._v("I am expected to graduate in June 2024 and seeking job opportunities worldwide. Please feel free to contact me if you are interested!")]),e._v(" "),t("h2",{attrs:{id:"education-experiences"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#education-experiences"}},[e._v("#")]),e._v(" Education & Experiences")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[e._v("Microsoft Research Asia")]),e._v(" "),t("span",{staticStyle:{color:"gray",float:"right"}},[e._v("May 2020 - Present")]),e._v(" "),t("br"),e._v("\nResearch Intern in Natural Language Computing Group")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Harbin Institute of Technology")]),e._v(" "),t("span",{staticStyle:{color:"gray",float:"right"}},[e._v("Aug 2019 - Present")]),e._v(" "),t("br"),e._v("\nPh.D. Student in Research Center for Social Computing and Information Retrieval")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Harvard University")]),e._v(" "),t("span",{staticStyle:{color:"gray",float:"right"}},[e._v("Aug 2018 - May 2019")]),e._v(" "),t("br"),e._v("\nResearch Intern in Data Systems Laboratory")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("National Chiao Tung University")]),e._v(" "),t("span",{staticStyle:{color:"gray",float:"right"}},[e._v("Aug 2017 - Jan 2018")]),e._v(" "),t("br"),e._v("\nExchange Student in Computer Science")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("Harbin Institute of Technology")]),e._v(" "),t("span",{staticStyle:{color:"gray",float:"right"}},[e._v("Aug 2015 - Jun 2019")]),e._v(" "),t("br"),e._v("\nB.S. in Computer Science and Technology")])])]),e._v(" "),t("h2",{attrs:{id:"selected-publications"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#selected-publications"}},[e._v("#")]),e._v(" Selected Publications")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://scholar.google.com/citations?user=XrZRIy0AAAAJ",target:"_blank",rel:"noopener noreferrer"}},[e._v("→ Full list (1000+ citations)"),t("OutboundLink")],1),e._v("  (*joint first author)")]),e._v(" "),t("ProjectCard",{attrs:{image:"/projects/BEATs.png",hideBorder:"true"}},[t("p",[t("strong",[e._v("BEATs: Audio Pre-Training with Acoustic Tokenizers")])]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Sanyuan Chen")]),e._v(", Yu Wu, Chengyi Wang, Shujie Liu, Daniel Tompkins, Zhuo Chen, Furu Wei")])]),e._v(" "),t("p",[t("strong",[e._v("Ranks 1st")]),e._v(" in the "),t("a",{attrs:{href:"https://paperswithcode.com/sota/audio-classification-on-audioset",target:"_blank",rel:"noopener noreferrer"}},[e._v("AudioSet leaderboard"),t("OutboundLink")],1),e._v("."),t("br"),e._v(" "),t("strong",[e._v("Ranks 1st")]),e._v(" in the "),t("a",{attrs:{href:"https://paperswithcode.com/sota/audio-classification-on-balanced-audio-set",target:"_blank",rel:"noopener noreferrer"}},[e._v("Balanced AudioSet leaderboard"),t("OutboundLink")],1),e._v("."),t("br"),e._v(" "),t("strong",[e._v("Ranks 1st")]),e._v(" in the "),t("a",{attrs:{href:"https://paperswithcode.com/sota/audio-classification-on-esc-50",target:"_blank",rel:"noopener noreferrer"}},[e._v("ESC-50 leaderboard"),t("OutboundLink")],1),e._v("."),t("br"),e._v(" "),t("strong",[e._v("Powers the winner")]),e._v(" in "),t("a",{attrs:{href:"https://dcase.community/challenge2023/task-automated-audio-captioning-results",target:"_blank",rel:"noopener noreferrer"}},[e._v("DCASE 2023 Automated Audio Captioning Challenge"),t("OutboundLink")],1),e._v("."),t("br"),e._v(" "),t("strong",[e._v("Powers all the top 5 systems")]),e._v(" in "),t("a",{attrs:{href:"https://dcase.community/challenge2023/task-sound-event-detection-with-weak-labels-and-synthetic-soundscapes",target:"_blank",rel:"noopener noreferrer"}},[e._v("DCASE 2023 Sound Event Detection Challenge"),t("OutboundLink")],1),e._v(".")]),e._v(" "),t("p",[e._v("["),t("a",{attrs:{href:"https://arxiv.org/abs/2212.09058",target:"_blank",rel:"noopener noreferrer"}},[e._v("Accepted in "),t("strong",[e._v("ICML 2023 oral")]),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://aka.ms/beats",target:"_blank",rel:"noopener noreferrer"}},[e._v("code"),t("OutboundLink")],1),e._v("]")])]),e._v(" "),t("ProjectCard",{attrs:{image:"/projects/VALLE.jpg",hideBorder:"true"}},[t("p",[t("strong",[e._v("Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers")])]),e._v(" "),t("p",[t("em",[e._v("Chengyi Wang*, "),t("strong",[e._v("Sanyuan Chen")]),e._v("*, Yu Wu*, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, Furu Wei")])]),e._v(" "),t("p",[t("strong",[e._v("VALL-E")]),e._v(" can synthesize speech with "),t("strong",[e._v("anyone's voice")]),e._v(" from just "),t("strong",[e._v("3 seconds")]),e._v(" of audio."),t("br"),e._v(" "),t("strong",[e._v("VALL-E X")]),e._v(" can help anyone speak a "),t("strong",[e._v("foreign language")]),e._v(" without an accent."),t("br"),e._v(" "),t("strong",[e._v("State-of-the-art")]),e._v(" zero-shot TTS system with "),t("strong",[e._v("in-context learning")]),e._v(" capabilities."),t("br"),e._v(" "),t("strong",[e._v("Wins")]),e._v(" the "),t("a",{attrs:{href:"https://twitter.com/Netexplo/status/1648330944632193024",target:"_blank",rel:"noopener noreferrer"}},[e._v("UNESCO Netexplo Innovation Award 2023"),t("OutboundLink")],1),e._v(" ("),t("strong",[e._v("top 10")]),e._v(" out of over 3000).")]),e._v(" "),t("p",[e._v("["),t("a",{attrs:{href:"https://arxiv.org/abs/2301.02111",target:"_blank",rel:"noopener noreferrer"}},[e._v("VALL-E paper"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://aka.ms/valle",target:"_blank",rel:"noopener noreferrer"}},[e._v("VALL-E demo"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://arxiv.org/abs/2303.03926",target:"_blank",rel:"noopener noreferrer"}},[e._v("VALL-E X paper"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://aka.ms/vallex",target:"_blank",rel:"noopener noreferrer"}},[e._v("VALL-E X demo"),t("OutboundLink")],1),e._v("]"),t("br"),e._v("\n["),t("a",{attrs:{href:"https://www.reddit.com/r/Futurology/comments/1090iix/microsofts_new_valle_ai_can_clone_your_voice_from/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Reddit Discussion"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://www.foxnews.com/tech/new-ai-simulate-voice-3-seconds-audio",target:"_blank",rel:"noopener noreferrer"}},[e._v("Fox News"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://news.yahoo.com/microsofts-ai-tool-just-needs-155500925.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Yahoo! News"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://arstechnica.com/information-technology/2023/01/microsofts-new-ai-can-simulate-anyones-voice-with-3-seconds-of-audio/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Ars Technica News"),t("OutboundLink")],1),e._v("]")])]),e._v(" "),t("ProjectCard",{attrs:{image:"/projects/SpeechLM.jpg",hideBorder:"true"}},[t("p",[t("strong",[e._v("SpeechLM: Enhanced Speech Pre-Training with Unpaired Textual Data")])]),e._v(" "),t("p",[t("em",[e._v("Ziqiang Zhang*, "),t("strong",[e._v("Sanyuan Chen")]),e._v("*, Long Zhou*, Yu Wu, Shuo Ren, Shujie Liu, Zhuoyuan Yao, Xun Gong, Lirong Dai, Jinyu Li, Furu Wei")])]),e._v(" "),t("p",[t("strong",[e._v("12% relative WER reduction")]),e._v(" over data2vec with only "),t("strong",[e._v("400K")]),e._v(" unlabeled text.")]),e._v(" "),t("p",[e._v("["),t("a",{attrs:{href:"https://arxiv.org/abs/2209.15329",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://aka.ms/speechlm",target:"_blank",rel:"noopener noreferrer"}},[e._v("code"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/579803231",target:"_blank",rel:"noopener noreferrer"}},[e._v("blog"),t("OutboundLink")],1),e._v("]")])]),e._v(" "),t("ProjectCard",{attrs:{image:"/projects/WavLM.png",hideBorder:"true"}},[t("p",[t("strong",[e._v("WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing")])]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Sanyuan Chen")]),e._v(", Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Xiangzhan Yu, Furu Wei")])]),e._v(" "),t("p",[t("strong",[e._v("Ranks 1st")]),e._v(" in the "),t("a",{attrs:{href:"https://superbbenchmark.org/leaderboard",target:"_blank",rel:"noopener noreferrer"}},[e._v("SUPERB leaderboard"),t("OutboundLink")],1),e._v(" and "),t("a",{attrs:{href:"https://superbbenchmark.org/",target:"_blank",rel:"noopener noreferrer"}},[e._v("SLT2022 SUPERB Challenge"),t("OutboundLink")],1),e._v("."),t("br"),e._v(" "),t("strong",[e._v("Ranks 1st")]),e._v(" on "),t("a",{attrs:{href:"https://competitions.codalab.org/competitions/34066#results",target:"_blank",rel:"noopener noreferrer"}},[e._v("VoxSRC 2021 speaker verification permanent leaderboard"),t("OutboundLink")],1),e._v(".\n"),t("strong",[e._v("Powers all the top 3 systems")]),e._v(" in "),t("a",{attrs:{href:"http://mm.kaist.ac.kr/datasets/voxceleb/voxsrc/interspeech2022.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("VoxSRC 2022 speaker verification challenge"),t("OutboundLink")],1),e._v(".")]),e._v(" "),t("p",[e._v("["),t("a",{attrs:{href:"https://ieeexplore.ieee.org/document/9814838",target:"_blank",rel:"noopener noreferrer"}},[e._v("Accepted in J-STSP"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://aka.ms/wavlm",target:"_blank",rel:"noopener noreferrer"}},[e._v("code"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://huggingface.co/spaces/microsoft/wavlm-speaker-verification",target:"_blank",rel:"noopener noreferrer"}},[e._v("demo"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://www.msra.cn/zh-cn/news/features/wavlm",target:"_blank",rel:"noopener noreferrer"}},[e._v("blog"),t("OutboundLink")],1),e._v("]\n["),t("a",{attrs:{href:"https://huggingface.co/microsoft/wavlm-large",target:"_blank",rel:"noopener noreferrer"}},[e._v("HuggingFace API"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://pytorch.org/audio/main/index.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("TorchAudio API"),t("OutboundLink")],1),e._v("]")])]),e._v(" "),t("ProjectCard",{attrs:{image:"/projects/UniSpeech-SAT.png",hideBorder:"true"}},[t("p",[t("strong",[e._v("UniSpeech-SAT: Universal Speech Representation Learning with Speaker Aware Pre-Training")])]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Sanyuan Chen")]),e._v(", Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu")])]),e._v(" "),t("p",[e._v("["),t("a",{attrs:{href:"https://ieeexplore.ieee.org/abstract/document/9747077",target:"_blank",rel:"noopener noreferrer"}},[e._v("Accepted in "),t("strong",[e._v("ICASSP 2022")]),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://github.com/microsoft/UniSpeech",target:"_blank",rel:"noopener noreferrer"}},[e._v("code"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://huggingface.co/spaces/microsoft/unispeech-speaker-verification",target:"_blank",rel:"noopener noreferrer"}},[e._v("demo"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://sigport.org/documents/unispeech-sat-universal-speech-representation-learning-speaker-aware-pre-training-0",target:"_blank",rel:"noopener noreferrer"}},[e._v("slides"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://sigport.org/documents/unispeech-sat-universal-speech-representation-learning-speaker-aware-pre-training",target:"_blank",rel:"noopener noreferrer"}},[e._v("poster"),t("OutboundLink")],1),e._v("]")])]),e._v(" "),t("ProjectCard",{attrs:{image:"/projects/Why_SSL2ASR_benifit_SID.png",hideBorder:"true"}},[t("p",[t("strong",[e._v("Why does Self-Supervised Learning for Speech Recognition Benefit Speaker Recognition?")])]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Sanyuan Chen")]),e._v(", Yu Wu, Chengyi Wang, Shujie Liu, Zhuo Chen, Peidong Wang, Gang Liu, Jinyu Li, Jian Wu, Xiangzhan Yu, Furu Wei")])]),e._v(" "),t("p",[e._v("["),t("a",{attrs:{href:"https://arxiv.org/pdf/2204.12765",target:"_blank",rel:"noopener noreferrer"}},[e._v("Accepted in INTERSPEECH 2022"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://github.com/microsoft/UniSpeech",target:"_blank",rel:"noopener noreferrer"}},[e._v("code"),t("OutboundLink")],1),e._v("]")])]),e._v(" "),t("ProjectCard",{attrs:{image:"/projects/Conformer.png",hideBorder:"true"}},[t("p",[t("strong",[e._v("Continuous Speech Separation with Conformer")])]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Sanyuan Chen")]),e._v(", Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Xiangzhan Yu, Furu Wei")])]),e._v(" "),t("p",[t("strong",[e._v("Ranks 1st")]),e._v(" in the "),t("a",{attrs:{href:"https://competitions.codalab.org/competitions/26357#results",target:"_blank",rel:"noopener noreferrer"}},[e._v("VoxCeleb Speaker Recognition Challenge 2020"),t("OutboundLink")],1),e._v("."),t("br"),e._v(" "),t("strong",[e._v("Shipped")]),e._v(" in the Microsoft Conversation Transcription Service.")]),e._v(" "),t("p",[e._v("["),t("a",{attrs:{href:"https://ieeexplore.ieee.org/document/9413423",target:"_blank",rel:"noopener noreferrer"}},[e._v("Accepted in "),t("strong",[e._v("ICASSP 2021")]),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://github.com/Sanyuan-Chen/CSS_with_Conformer",target:"_blank",rel:"noopener noreferrer"}},[e._v("code"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://www.youtube.com/watch?v=WRfPBnWc2qQ&t=3s",target:"_blank",rel:"noopener noreferrer"}},[e._v("demo"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://sigport.org/documents/continuous-speech-separation-conformer-0",target:"_blank",rel:"noopener noreferrer"}},[e._v("slides"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://sigport.org/documents/continuous-speech-separation-conformer",target:"_blank",rel:"noopener noreferrer"}},[e._v("poster"),t("OutboundLink")],1),e._v("]")])]),e._v(" "),t("ProjectCard",{attrs:{image:"/projects/FastSS_with_TS.png",hideBorder:"true"}},[t("p",[t("strong",[e._v("Ultra Fast Speech Separation Model with Teacher Student Learning")])]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Sanyuan Chen")]),e._v(", Yu Wu, Zhuo Chen, Jian Wu, Takuya Yoshioka, Shujie Liu, Jinyu Li, Xiangzhan Yu")])]),e._v(" "),t("p",[t("strong",[e._v("Shipped")]),e._v(" in the Microsoft Conversation Transcription Service.")]),e._v(" "),t("p",[e._v("["),t("a",{attrs:{href:"https://www.isca-speech.org/archive/pdfs/interspeech_2021/chen21l_interspeech.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Accepted in "),t("strong",[e._v("INTERSPEECH 2021")]),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://github.com/Sanyuan-Chen/CSS_with_TSTransformer",target:"_blank",rel:"noopener noreferrer"}},[e._v("code"),t("OutboundLink")],1),e._v("]")])]),e._v(" "),t("ProjectCard",{attrs:{image:"/projects/EETransformer.png",hideBorder:"true"}},[t("p",[t("strong",[e._v("Don’t shoot butterfly with rifles: Multi-channel Continuous Speech Separation with Early Exit Transformer")])]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Sanyuan Chen")]),e._v(", Yu Wu, Zhuo Chen, Takuya Yoshioka, Shujie Liu, Jinyu Li, Xiangzhan Yu")])]),e._v(" "),t("p",[e._v("["),t("a",{attrs:{href:"https://ieeexplore.ieee.org/document/9413933",target:"_blank",rel:"noopener noreferrer"}},[e._v("Accepted in "),t("strong",[e._v("ICASSP 2021")]),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://github.com/Sanyuan-Chen/CSS_with_EETransformer",target:"_blank",rel:"noopener noreferrer"}},[e._v("code"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://sigport.org/documents/dont-shoot-butterfly-rifles-multi-channel-continuous-speech-separation-early-exit",target:"_blank",rel:"noopener noreferrer"}},[e._v("slides"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://sigport.org/documents/dont-shoot-butterfly-rifles-multi-channel-continuous-speech-separation-early-exit-0",target:"_blank",rel:"noopener noreferrer"}},[e._v("poster"),t("OutboundLink")],1),e._v("]")])]),e._v(" "),t("ProjectCard",{attrs:{image:"/projects/C2CGenDA.png",hideBorder:"true"}},[t("p",[t("strong",[e._v("C2C-GenDA: Cluster-to-Cluster Generation for Data Augmentation of Slot Filling")])]),e._v(" "),t("p",[t("em",[e._v("Yutai Hou*, "),t("strong",[e._v("Sanyuan Chen")]),e._v("*, Wanxiang Che, Cheng Chen, Ting Liu")])]),e._v(" "),t("p",[e._v("["),t("a",{attrs:{href:"https://www.aaai.org/AAAI21Papers/AAAI-10147.HouY.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Accepted in "),t("strong",[e._v("AAAI 2021")]),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://github.com/Sanyuan-Chen/C2C-DA",target:"_blank",rel:"noopener noreferrer"}},[e._v("code"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://slideslive.com/38949311/c2cgenda-clustertocluster-generation-for-data-augmentation-of-slot-filling",target:"_blank",rel:"noopener noreferrer"}},[e._v("video"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://mp.weixin.qq.com/s?__biz=MzIxMjAzNDY5Mg==&mid=2650800309&idx=1&sn=90dd2ccdcaa28f5fa1557bcd5f34e07c&chksm=8cb8955ebbcf1c48de9954c6a63567ff1b961b725b2ab8e466d522eba148570f321c684fe2af&mpshare=1&scene=1&srcid=0802Ug7wPMER4lbWqFsqlkZy&sharer_sharetime=1659438504889&sharer_shareid=dca445c61c90f52c6c79e7e2c470b7cf&exportkey=A9toVlohSOljuoH17I2mf%2BI%3D&acctmode=0&pass_ticket=vUtNVU7gzDGC1Php4jmJuRlmPfhpOTcqwWiIt3HikG%2FRA4BW%2BzIM81iXEWssJsso&wx_header=0#rd",target:"_blank",rel:"noopener noreferrer"}},[e._v("blog"),t("OutboundLink")],1),e._v("]")])]),e._v(" "),t("ProjectCard",{attrs:{image:"/projects/RecAdam.png",hideBorder:"true"}},[t("p",[t("strong",[e._v("Recall and learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting")])]),e._v(" "),t("p",[t("em",[t("strong",[e._v("Sanyuan Chen")]),e._v(", Yutai Hou, Yiming Cui, Wanxiang Che, Ting Liu, Xiangzhan Yu")])]),e._v(" "),t("p",[e._v("["),t("a",{attrs:{href:"https://aclanthology.org/2020.emnlp-main.634",target:"_blank",rel:"noopener noreferrer"}},[e._v("Accepted in "),t("strong",[e._v("EMNLP 2020")]),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://github.com/Sanyuan-Chen/RecAdam",target:"_blank",rel:"noopener noreferrer"}},[e._v("code"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://slideslive.com/38938976/recall-and-learn-finetuning-deep-pretrained-language-models-with-less-forgetting",target:"_blank",rel:"noopener noreferrer"}},[e._v("video"),t("OutboundLink")],1),e._v("] ["),t("a",{attrs:{href:"https://mp.weixin.qq.com/s?__biz=MzIxMjAzNDY5Mg==&mid=2650799813&idx=1&sn=327e7b5541eac775a3e78e635d8252dc&exportkey=AxqgQNAq7%2F4SRd0HHZo3P5U%3D&acctmode=0&pass_ticket=vUtNVU7gzDGC1Php4jmJuRlmPfhpOTcqwWiIt3HikG%2FRA4BW%2BzIM81iXEWssJsso&wx_header=0",target:"_blank",rel:"noopener noreferrer"}},[e._v("blog"),t("OutboundLink")],1),e._v("]")])])],1)}),[],!1,null,null,null);r.default=a.exports}}]);