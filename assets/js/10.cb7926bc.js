(window.webpackJsonp=window.webpackJsonp||[]).push([[10],{281:function(e,r,n){"use strict";n.r(r);var t=n(13),a=Object(t.a)({},(function(){var e=this,r=e.$createElement,n=e._self._c||r;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("ProfileSection",{attrs:{frontmatter:e.$page.frontmatter}}),e._v(" "),n("h2",{attrs:{id:"about-me"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#about-me"}},[e._v("#")]),e._v(" About Me")]),e._v(" "),n("p",[e._v("Hi there! I am a joint Ph.D. student of "),n("a",{attrs:{href:"http://en.hit.edu.cn/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Harbin Institute of Technology"),n("OutboundLink")],1),e._v("  and "),n("a",{attrs:{href:"https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Microsoft Research Asia"),n("OutboundLink")],1),e._v(", supervised by Prof. Xiangzhan Yu and Dr. Ming Zhou.\nI obtained my B.S. degree at Harbin Institute of Technology, supervised by "),n("a",{attrs:{href:"http://ir.hit.edu.cn/~car/zh/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Prof. Wanxiang Che"),n("OutboundLink")],1),e._v(".\nCurrently, I am working as a research intern at Natural Language Computing Group in MSRA, mentored by "),n("a",{attrs:{href:"https://www.microsoft.com/en-us/research/people/yuwu1/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Dr. Yu Wu"),n("OutboundLink")],1),e._v(" and "),n("a",{attrs:{href:"https://www.microsoft.com/en-us/research/people/shujliu/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Dr. Shujie Liu"),n("OutboundLink")],1),e._v(".\nMy research interests include self-supervised learning, speech processing and spoken language processing.")]),e._v(" "),n("h2",{attrs:{id:"education-experiences"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#education-experiences"}},[e._v("#")]),e._v(" Education & Experiences")]),e._v(" "),n("ul",[n("li",[n("p",[n("strong",[e._v("Microsoft Research Asia")]),e._v(" "),n("span",{staticStyle:{color:"gray",float:"right"}},[e._v("May 2020 - Present")]),e._v(" "),n("br"),e._v("\nResearch Intern in Natural Language Computing Group")])]),e._v(" "),n("li",[n("p",[n("strong",[e._v("Harbin Institute of Technology")]),e._v(" "),n("span",{staticStyle:{color:"gray",float:"right"}},[e._v("Aug 2019 - Present")]),e._v(" "),n("br"),e._v("\nPh.D. Student in Research Center for Social Computing and Information Retrieval")])]),e._v(" "),n("li",[n("p",[n("strong",[e._v("Harvard University")]),e._v(" "),n("span",{staticStyle:{color:"gray",float:"right"}},[e._v("Aug 2018 - May 2019")]),e._v(" "),n("br"),e._v("\nUndergraduate Research Intern in Data Systems Laboratory")])]),e._v(" "),n("li",[n("p",[n("strong",[e._v("National Chiao Tung University")]),e._v(" "),n("span",{staticStyle:{color:"gray",float:"right"}},[e._v("Aug 2017 - Jan 2018")]),e._v(" "),n("br"),e._v("\nExchange Student in Computer Science")])]),e._v(" "),n("li",[n("p",[n("strong",[e._v("Harbin Institute of Technology")]),e._v(" "),n("span",{staticStyle:{color:"gray",float:"right"}},[e._v("Aug 2015 - Jun 2019")]),e._v(" "),n("br"),e._v("\nB.S. in Computer Science and Technology")])])]),e._v(" "),n("h2",{attrs:{id:"selected-publications"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#selected-publications"}},[e._v("#")]),e._v(" Selected Publications")]),e._v(" "),n("p",[n("a",{attrs:{href:"https://scholar.google.com/citations?user=XrZRIy0AAAAJ",target:"_blank",rel:"noopener noreferrer"}},[e._v("â†’ Full list"),n("OutboundLink")],1),e._v("  (*joint first author)")]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/BEATs.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("BEATs: Audio Pre-Training with Acoustic Tokenizers")])]),e._v(" "),n("p",[n("em",[n("strong",[e._v("Sanyuan Chen")]),e._v(", Yu Wu, Chengyi Wang, Shujie Liu, Daniel Tompkins, Zhuo Chen, Furu Wei")])]),e._v(" "),n("p",[n("strong",[e._v("Ranks 1st")]),e._v(" in the "),n("a",{attrs:{href:"https://paperswithcode.com/sota/audio-classification-on-audioset",target:"_blank",rel:"noopener noreferrer"}},[e._v("AudioSet leaderboard"),n("OutboundLink")],1),e._v("."),n("br"),e._v(" "),n("strong",[e._v("Ranks 1st")]),e._v(" in the "),n("a",{attrs:{href:"https://paperswithcode.com/sota/audio-classification-on-balanced-audio-set",target:"_blank",rel:"noopener noreferrer"}},[e._v("Balanced AudioSet leaderboard"),n("OutboundLink")],1),e._v("."),n("br"),e._v(" "),n("strong",[e._v("Ranks 1st")]),e._v(" in the "),n("a",{attrs:{href:"https://paperswithcode.com/sota/audio-classification-on-esc-50",target:"_blank",rel:"noopener noreferrer"}},[e._v("ESC-50 leaderboard"),n("OutboundLink")],1),e._v(".")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://arxiv.org/abs/2212.09058",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://aka.ms/beats",target:"_blank",rel:"noopener noreferrer"}},[e._v("code"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/VALLE.jpg",hideBorder:"true"}},[n("p",[n("strong",[e._v("Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers")])]),e._v(" "),n("p",[n("em",[e._v("Chengyi Wang*, "),n("strong",[e._v("Sanyuan Chen")]),e._v("*, Yu Wu*, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, Furu Wei")])]),e._v(" "),n("p",[n("strong",[e._v("State-of-the-art")]),e._v(" zero-shot TTS system with "),n("strong",[e._v("in-context learning")]),e._v(" capabilities.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://arxiv.org/abs/2301.02111",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://aka.ms/valle",target:"_blank",rel:"noopener noreferrer"}},[e._v("demo"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/SpeechLM.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("SpeechLM: Enhanced Speech Pre-Training with Unpaired Textual Data")])]),e._v(" "),n("p",[n("em",[e._v("Ziqiang Zhang*, "),n("strong",[e._v("Sanyuan Chen")]),e._v("*, Long Zhou*, Yu Wu, Shuo Ren, Shujie Liu, Zhuoyuan Yao, Xun Gong, Lirong Dai, Jinyu Li, Furu Wei")])]),e._v(" "),n("p",[n("strong",[e._v("16% relative WER reduction")]),e._v(" over data2vec with only "),n("strong",[e._v("10K")]),e._v(" text sentences.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://arxiv.org/abs/2209.15329",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://aka.ms/speechlm",target:"_blank",rel:"noopener noreferrer"}},[e._v("code"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/WavLM.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing")])]),e._v(" "),n("p",[n("em",[n("strong",[e._v("Sanyuan Chen")]),e._v(", Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Xiangzhan Yu, Furu Wei")])]),e._v(" "),n("p",[n("strong",[e._v("Ranks 1st")]),e._v(" in the "),n("a",{attrs:{href:"https://superbbenchmark.org/leaderboard",target:"_blank",rel:"noopener noreferrer"}},[e._v("SUPERB leaderboard"),n("OutboundLink")],1),e._v(" and "),n("a",{attrs:{href:"https://superbbenchmark.org/",target:"_blank",rel:"noopener noreferrer"}},[e._v("SLT2022 SUPERB Challenge"),n("OutboundLink")],1),e._v("."),n("br"),e._v(" "),n("strong",[e._v("Ranks 1st")]),e._v(" on "),n("a",{attrs:{href:"https://competitions.codalab.org/competitions/34066#results",target:"_blank",rel:"noopener noreferrer"}},[e._v("VoxSRC 2021 speaker verification permanent leaderboard"),n("OutboundLink")],1),e._v(".")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://ieeexplore.ieee.org/document/9814838",target:"_blank",rel:"noopener noreferrer"}},[e._v("Accepted in J-STSP"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://aka.ms/wavlm",target:"_blank",rel:"noopener noreferrer"}},[e._v("code"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://huggingface.co/spaces/microsoft/wavlm-speaker-verification",target:"_blank",rel:"noopener noreferrer"}},[e._v("demo"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://www.msra.cn/zh-cn/news/features/wavlm",target:"_blank",rel:"noopener noreferrer"}},[e._v("blog"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/UniSpeech-SAT.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("UniSpeech-SAT: Universal Speech Representation Learning with Speaker Aware Pre-Training")])]),e._v(" "),n("p",[n("em",[n("strong",[e._v("Sanyuan Chen")]),e._v(", Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu")])]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://ieeexplore.ieee.org/abstract/document/9747077",target:"_blank",rel:"noopener noreferrer"}},[e._v("Accepted in "),n("strong",[e._v("ICASSP 2022")]),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://github.com/microsoft/UniSpeech",target:"_blank",rel:"noopener noreferrer"}},[e._v("code"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://huggingface.co/spaces/microsoft/unispeech-speaker-verification",target:"_blank",rel:"noopener noreferrer"}},[e._v("demo"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://sigport.org/documents/unispeech-sat-universal-speech-representation-learning-speaker-aware-pre-training-0",target:"_blank",rel:"noopener noreferrer"}},[e._v("slides"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://sigport.org/documents/unispeech-sat-universal-speech-representation-learning-speaker-aware-pre-training",target:"_blank",rel:"noopener noreferrer"}},[e._v("poster"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/Why_SSL2ASR_benifit_SID.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("Why does Self-Supervised Learning for Speech Recognition Benefit Speaker Recognition?")])]),e._v(" "),n("p",[n("em",[n("strong",[e._v("Sanyuan Chen")]),e._v(", Yu Wu, Chengyi Wang, Shujie Liu, Zhuo Chen, Peidong Wang, Gang Liu, Jinyu Li, Jian Wu, Xiangzhan Yu, Furu Wei")])]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://arxiv.org/pdf/2204.12765",target:"_blank",rel:"noopener noreferrer"}},[e._v("Accepted in INTERSPEECH 2022"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://github.com/microsoft/UniSpeech",target:"_blank",rel:"noopener noreferrer"}},[e._v("code"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/Conformer.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("Continuous Speech Separation with Conformer")])]),e._v(" "),n("p",[n("em",[n("strong",[e._v("Sanyuan Chen")]),e._v(", Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Xiangzhan Yu, Furu Wei")])]),e._v(" "),n("p",[n("strong",[e._v("Ranks 1st")]),e._v(" in the "),n("a",{attrs:{href:"https://competitions.codalab.org/competitions/26357#results",target:"_blank",rel:"noopener noreferrer"}},[e._v("VoxCeleb Speaker Recognition Challenge 2020"),n("OutboundLink")],1),e._v(".")]),e._v(" "),n("p",[n("strong",[e._v("Shipped")]),e._v(" in the Microsoft Conversation Transcription Service.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://ieeexplore.ieee.org/document/9413423",target:"_blank",rel:"noopener noreferrer"}},[e._v("Accepted in "),n("strong",[e._v("ICASSP 2021")]),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://github.com/Sanyuan-Chen/CSS_with_Conformer",target:"_blank",rel:"noopener noreferrer"}},[e._v("code"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://www.youtube.com/watch?v=WRfPBnWc2qQ&t=3s",target:"_blank",rel:"noopener noreferrer"}},[e._v("demo"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://sigport.org/documents/continuous-speech-separation-conformer-0",target:"_blank",rel:"noopener noreferrer"}},[e._v("slides"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://sigport.org/documents/continuous-speech-separation-conformer",target:"_blank",rel:"noopener noreferrer"}},[e._v("poster"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/FastSS_with_TS.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("Ultra Fast Speech Separation Model with Teacher Student Learning")])]),e._v(" "),n("p",[n("em",[n("strong",[e._v("Sanyuan Chen")]),e._v(", Yu Wu, Zhuo Chen, Jian Wu, Takuya Yoshioka, Shujie Liu, Jinyu Li, Xiangzhan Yu")])]),e._v(" "),n("p",[n("strong",[e._v("Shipped")]),e._v(" in the Microsoft Conversation Transcription Service.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://www.isca-speech.org/archive/pdfs/interspeech_2021/chen21l_interspeech.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Accepted in "),n("strong",[e._v("INTERSPEECH 2021")]),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://github.com/Sanyuan-Chen/CSS_with_TSTransformer",target:"_blank",rel:"noopener noreferrer"}},[e._v("code"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/EETransformer.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("Donâ€™t shoot butterfly with rifles: Multi-channel Continuous Speech Separation with Early Exit Transformer")])]),e._v(" "),n("p",[n("em",[n("strong",[e._v("Sanyuan Chen")]),e._v(", Yu Wu, Zhuo Chen, Takuya Yoshioka, Shujie Liu, Jinyu Li, Xiangzhan Yu")])]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://ieeexplore.ieee.org/document/9413933",target:"_blank",rel:"noopener noreferrer"}},[e._v("Accepted in "),n("strong",[e._v("ICASSP 2021")]),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://github.com/Sanyuan-Chen/CSS_with_EETransformer",target:"_blank",rel:"noopener noreferrer"}},[e._v("code"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://sigport.org/documents/dont-shoot-butterfly-rifles-multi-channel-continuous-speech-separation-early-exit",target:"_blank",rel:"noopener noreferrer"}},[e._v("slides"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://sigport.org/documents/dont-shoot-butterfly-rifles-multi-channel-continuous-speech-separation-early-exit-0",target:"_blank",rel:"noopener noreferrer"}},[e._v("poster"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/C2CGenDA.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("C2C-GenDA: Cluster-to-Cluster Generation for Data Augmentation of Slot Filling")])]),e._v(" "),n("p",[n("em",[e._v("Yutai Hou*, "),n("strong",[e._v("Sanyuan Chen")]),e._v("*, Wanxiang Che, Cheng Chen, Ting Liu")])]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://www.aaai.org/AAAI21Papers/AAAI-10147.HouY.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Accepted in "),n("strong",[e._v("AAAI 2021")]),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://github.com/Sanyuan-Chen/C2C-DA",target:"_blank",rel:"noopener noreferrer"}},[e._v("code"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://slideslive.com/38949311/c2cgenda-clustertocluster-generation-for-data-augmentation-of-slot-filling",target:"_blank",rel:"noopener noreferrer"}},[e._v("video"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://mp.weixin.qq.com/s?__biz=MzIxMjAzNDY5Mg==&mid=2650800309&idx=1&sn=90dd2ccdcaa28f5fa1557bcd5f34e07c&chksm=8cb8955ebbcf1c48de9954c6a63567ff1b961b725b2ab8e466d522eba148570f321c684fe2af&mpshare=1&scene=1&srcid=0802Ug7wPMER4lbWqFsqlkZy&sharer_sharetime=1659438504889&sharer_shareid=dca445c61c90f52c6c79e7e2c470b7cf&exportkey=A9toVlohSOljuoH17I2mf%2BI%3D&acctmode=0&pass_ticket=vUtNVU7gzDGC1Php4jmJuRlmPfhpOTcqwWiIt3HikG%2FRA4BW%2BzIM81iXEWssJsso&wx_header=0#rd",target:"_blank",rel:"noopener noreferrer"}},[e._v("blog"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/RecAdam.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("Recall and learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting")])]),e._v(" "),n("p",[n("em",[n("strong",[e._v("Sanyuan Chen")]),e._v(", Yutai Hou, Yiming Cui, Wanxiang Che, Ting Liu, Xiangzhan Yu")])]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://aclanthology.org/2020.emnlp-main.634",target:"_blank",rel:"noopener noreferrer"}},[e._v("Accepted in "),n("strong",[e._v("EMNLP 2020")]),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://github.com/Sanyuan-Chen/RecAdam",target:"_blank",rel:"noopener noreferrer"}},[e._v("code"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://slideslive.com/38938976/recall-and-learn-finetuning-deep-pretrained-language-models-with-less-forgetting",target:"_blank",rel:"noopener noreferrer"}},[e._v("video"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://mp.weixin.qq.com/s?__biz=MzIxMjAzNDY5Mg==&mid=2650799813&idx=1&sn=327e7b5541eac775a3e78e635d8252dc&exportkey=AxqgQNAq7%2F4SRd0HHZo3P5U%3D&acctmode=0&pass_ticket=vUtNVU7gzDGC1Php4jmJuRlmPfhpOTcqwWiIt3HikG%2FRA4BW%2BzIM81iXEWssJsso&wx_header=0",target:"_blank",rel:"noopener noreferrer"}},[e._v("blog"),n("OutboundLink")],1),e._v("]")])])],1)}),[],!1,null,null,null);r.default=a.exports}}]);